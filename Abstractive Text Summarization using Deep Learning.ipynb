{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 568411 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568411 non-null  int64 \n",
      " 1   ProductId               568411 non-null  object\n",
      " 2   UserId                  568411 non-null  object\n",
      " 3   ProfileName             568411 non-null  object\n",
      " 4   HelpfulnessNumerator    568411 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568411 non-null  int64 \n",
      " 6   Score                   568411 non-null  int64 \n",
      " 7   Time                    568411 non-null  int64 \n",
      " 8   Summary                 568411 non-null  object\n",
      " 9   Text                    568411 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 47.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/amazon-fine-food-reviews.csv')\n",
    "data.drop_duplicates(inplace=True) # dropping duplicates\n",
    "data.dropna(axis=0, inplace = True) # dropping empty rows\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp/ipykernel_13296/3557384794.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to carry out contraction expansions\n",
    "def cleaner(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"y\\'all\", \"you all\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(column_name):\n",
    "    preprocessed_text = []\n",
    "    for sentence in tqdm(data[column_name].values):\n",
    "        try:\n",
    "            sentence = sentence.lower() # converting all letters to lowercase\n",
    "            sentence = re.sub(r\"http\\S+\", \"\", sentence) # removing urls\n",
    "            sentence = BeautifulSoup(sentence, 'lxml').get_text() # removing any html tags\n",
    "            sentence = re.sub(r'\\([^)]*\\)', '', sentence) # removing any text inside parantheses\n",
    "            sentence = cleaner(sentence) # contraction expansions\n",
    "            sentence = re.sub(r\"'s\\b\",\"\", sentence) # removing apostrophes\n",
    "            sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip() # whitespace\n",
    "            sentence = re.sub('[^A-Za-z]+', ' ', sentence) # removing special characters, punctuation marks\n",
    "            preprocessed_text.append(sentence.strip())\n",
    "        except:\n",
    "            preprocessed_text.append('')\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8153/100000 [00:01<00:20, 4412.94it/s]C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"...\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100000/100000 [00:22<00:00, 4439.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries have been cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned Summary'] = clean('Summary')\n",
    "print(\"Summaries have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:32<00:00, 3102.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts have been cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned Text'] = clean('Text')\n",
    "print(\"Texts have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data/Cleaned Data.csv', index = False) # exporting as CSV for external use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Summary          100000 non-null  object\n",
      " 1   Text             100000 non-null  object\n",
      " 2   Cleaned Summary  100000 non-null  object\n",
      " 3   Cleaned Text     100000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/Cleaned Data.csv')\n",
    "data = data.replace(np.nan, '', regex = True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meat and it smells better my labrador is finicky and she appreciates this product better than most\n",
      "Summary: good quality dog food\n",
      "\n",
      "\n",
      "Review: product arrived labeled as jumbo salted peanuts the peanuts were actually small sized unsalted not sure if this was an error or if the vendor intended to represent the product as jumbo\n",
      "Summary: not as advertised\n",
      "\n",
      "\n",
      "Review: this is a confection that has been around a few centuries it is a light pillowy citrus gelatin with nuts in this case filberts and it is cut into tiny squares and then liberally coated with powdered sugar and it is a tiny mouthful of heaven not too chewy and very flavorful i highly recommend this yummy treat if you are familiar with the story of c s lewis the lion the witch and the wardrobe this is the treat that seduces edmund into selling out his brother and sisters to the witch\n",
      "Summary: delight says it all\n",
      "\n",
      "\n",
      "Review: if you are looking for the secret ingredient in robitussin i believe i have found it i got this in addition to the root beer extract i ordered and made some cherry soda the flavor is very medicinal\n",
      "Summary: cough medicine\n",
      "\n",
      "\n",
      "Review: great taffy at a great price there was a wide assortment of yummy taffy delivery was very quick if your a taffy lover this is a deal\n",
      "Summary: great taffy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['Cleaned Text'][i])\n",
    "    print(\"Summary:\",data['Cleaned Summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97373\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in data['Cleaned Summary']:\n",
    "    if(len(i.split()) <= 10):\n",
    "        count = count + 1\n",
    "print(count / len(data['Cleaned Summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_summary_len = 10\n",
    "max_text_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['Cleaned Text'])\n",
    "cleaned_summary=np.array(data['Cleaned Summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "data=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['summary'] = data['summary'].apply(lambda x : 'START '+ x + ' END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['text'], data['summary'], test_size=0.2, random_state=0, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total % of rare words in vocabulary:  64.32498772704959\n",
      "Total coverage of rare words in vocabulary:  1.6614259054788865\n",
      "Size of reviews vocabulary:  7268\n"
     ]
    }
   ],
   "source": [
    "# initializing a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# any word whose word count falls below 4 would be considered a rare word\n",
    "threshold = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    total_frequency = total_frequency + value\n",
    "    if(value < threshold):\n",
    "        count = count + 1\n",
    "        frequency = frequency + value\n",
    "    \n",
    "print(\"Total % of rare words in vocabulary: \", (count / total_count) * 100)\n",
    "print(\"Total coverage of rare words in vocabulary: \", (frequency / total_frequency) * 100)\n",
    "\n",
    "# preparing a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words = total_count - count) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# converting text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# padding zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen = max_text_len, padding = 'post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen = max_text_len, padding = 'post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "print(\"Size of reviews vocabulary: \", x_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 69.59027690765568\n",
      "Total Coverage of rare words: 4.157945920805057\n",
      "Size of summary vocabulary:  2428\n"
     ]
    }
   ],
   "source": [
    "# initialzing a tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# any word whose word count falls below 4 would be considered a rare word\n",
    "thresh = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    total_frequency = total_frequency + value\n",
    "    if(value < thresh):\n",
    "        count = count + 1\n",
    "        frequency = frequency + value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\", (count / total_count) * 100)\n",
    "print(\"Total Coverage of rare words:\", (frequency / total_frequency) * 100)\n",
    "\n",
    "# preparing a tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer(num_words = total_count-count) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# converting text sequences into integer sequences\n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# padding zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen = max_summary_len, padding = 'post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen = max_summary_len, padding = 'post')\n",
    "\n",
    "# size of vocabulary\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "print(\"Size of summary vocabulary: \", y_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)\n",
    "\n",
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 100)      726800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 300), (N 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 300), (N 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    242800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 300), (N 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 2428)   1459228     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,013,928\n",
      "Trainable params: 5,013,928\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape = (max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim, trainable = True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape = (None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout = 0.2)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name = 'attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis= -1, name = 'concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "135/135 [==============================] - 1247s 9s/step - loss: 2.5872 - val_loss: 2.2258\n",
      "Epoch 2/25\n",
      "135/135 [==============================] - 1504s 11s/step - loss: 2.2167 - val_loss: 2.1471\n",
      "Epoch 3/25\n",
      "135/135 [==============================] - 1536s 11s/step - loss: 2.1365 - val_loss: 2.0659\n",
      "Epoch 4/25\n",
      "135/135 [==============================] - 1532s 11s/step - loss: 2.0560 - val_loss: 2.0025\n",
      "Epoch 5/25\n",
      "135/135 [==============================] - 1386s 10s/step - loss: 1.9707 - val_loss: 1.9325\n",
      "Epoch 6/25\n",
      "135/135 [==============================] - 1423s 11s/step - loss: 1.8969 - val_loss: 1.8760\n",
      "Epoch 7/25\n",
      "135/135 [==============================] - 1524s 11s/step - loss: 1.8344 - val_loss: 1.8374\n",
      "Epoch 8/25\n",
      "135/135 [==============================] - 1452s 11s/step - loss: 1.7850 - val_loss: 1.8123\n",
      "Epoch 9/25\n",
      "135/135 [==============================] - 1555s 12s/step - loss: 1.7378 - val_loss: 1.7785\n",
      "Epoch 10/25\n",
      "135/135 [==============================] - 4182s 31s/step - loss: 1.6972 - val_loss: 1.7652\n",
      "Epoch 11/25\n",
      "135/135 [==============================] - 2540s 19s/step - loss: 1.6586 - val_loss: 1.7525\n",
      "Epoch 12/25\n",
      "135/135 [==============================] - 1569s 12s/step - loss: 1.6235 - val_loss: 1.7314\n",
      "Epoch 13/25\n",
      "135/135 [==============================] - 1631s 12s/step - loss: 1.5888 - val_loss: 1.7267\n",
      "Epoch 14/25\n",
      "135/135 [==============================] - 1587s 12s/step - loss: 1.5562 - val_loss: 1.7160\n",
      "Epoch 15/25\n",
      "135/135 [==============================] - 1592s 12s/step - loss: 1.5237 - val_loss: 1.7073\n",
      "Epoch 16/25\n",
      "135/135 [==============================] - 1512s 11s/step - loss: 1.4923 - val_loss: 1.7032\n",
      "Epoch 17/25\n",
      "135/135 [==============================] - 1470s 11s/step - loss: 1.4628 - val_loss: 1.7004\n",
      "Epoch 18/25\n",
      "135/135 [==============================] - 1471s 11s/step - loss: 1.4350 - val_loss: 1.6905\n",
      "Epoch 19/25\n",
      "135/135 [==============================] - 1488s 11s/step - loss: 1.4049 - val_loss: 1.6798\n",
      "Epoch 20/25\n",
      "135/135 [==============================] - 1491s 11s/step - loss: 1.3777 - val_loss: 1.6821\n",
      "Epoch 21/25\n",
      "135/135 [==============================] - 1465s 11s/step - loss: 1.3516 - val_loss: 1.6837\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_tr, y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:], epochs = 25, callbacks = [es], batch_size = 256, validation_data = ([x_val, y_val[:, :-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape = (max_text_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = [decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis = -1, name = 'concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        if(sampled_token != 'end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'end'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if((i != 0 and i != target_word_index['start']) and i != target_word_index['end']):\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if(i != 0):\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100):\n",
    "    print(\"Review:\", seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\", seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\", decode_sequence(x_val[i].reshape(1, max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/Text Summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06cb67651ce969e244f20f184e33f4ddb1106793adfe07cf7ddc131c95c4012f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
