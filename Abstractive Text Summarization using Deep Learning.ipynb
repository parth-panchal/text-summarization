{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 568411 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568411 non-null  int64 \n",
      " 1   ProductId               568411 non-null  object\n",
      " 2   UserId                  568411 non-null  object\n",
      " 3   ProfileName             568411 non-null  object\n",
      " 4   HelpfulnessNumerator    568411 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568411 non-null  int64 \n",
      " 6   Score                   568411 non-null  int64 \n",
      " 7   Time                    568411 non-null  int64 \n",
      " 8   Summary                 568411 non-null  object\n",
      " 9   Text                    568411 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 47.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/amazon-fine-food-reviews.csv')\n",
    "data.drop_duplicates(inplace=True) # dropping duplicates\n",
    "data.dropna(axis=0, inplace = True) # dropping empty rows\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp/ipykernel_13488/3557384794.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to carry out contraction expansions\n",
    "def cleaner(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"y\\'all\", \"you all\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(column_name):\n",
    "    preprocessed_text = []\n",
    "    for sentence in tqdm(data[column_name].values):\n",
    "        try:\n",
    "            sentence = sentence.lower() # converting all letters to lowercase\n",
    "            sentence = re.sub(r\"http\\S+\", \"\", sentence) # removing urls\n",
    "            sentence = BeautifulSoup(sentence, 'lxml').get_text() # removing any html tags\n",
    "            sentence = re.sub(r'\\([^)]*\\)', '', sentence) # removing any text inside parantheses\n",
    "            sentence = cleaner(sentence) # contraction expansions\n",
    "            sentence = re.sub(r\"'s\\b\",\"\", sentence) # removing apostrophes\n",
    "            sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip() # whitespace\n",
    "            sentence = re.sub('[^A-Za-z]+', ' ', sentence) # removing special characters, punctuation marks\n",
    "            preprocessed_text.append(sentence.strip())\n",
    "        except:\n",
    "            preprocessed_text.append('')\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8216/100000 [00:01<00:21, 4200.39it/s]C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"...\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100000/100000 [00:23<00:00, 4301.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries have been cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned Summary'] = clean('Summary')\n",
    "print(\"Summaries have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:36<00:00, 2719.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts have been cleaned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned Text'] = clean('Text')\n",
    "print(\"Texts have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data/Cleaned_Data.csv', index = False) # exporting as CSV for external use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Summary          100000 non-null  object\n",
      " 1   Text             100000 non-null  object\n",
      " 2   Cleaned Summary  100000 non-null  object\n",
      " 3   Cleaned Text     100000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Data/Cleaned_Data.csv')\n",
    "data = data.replace(np.nan, '', regex = True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meat and it smells better my labrador is finicky and she appreciates this product better than most\n",
      "Summary: good quality dog food\n",
      "\n",
      "\n",
      "Review: product arrived labeled as jumbo salted peanuts the peanuts were actually small sized unsalted not sure if this was an error or if the vendor intended to represent the product as jumbo\n",
      "Summary: not as advertised\n",
      "\n",
      "\n",
      "Review: this is a confection that has been around a few centuries it is a light pillowy citrus gelatin with nuts in this case filberts and it is cut into tiny squares and then liberally coated with powdered sugar and it is a tiny mouthful of heaven not too chewy and very flavorful i highly recommend this yummy treat if you are familiar with the story of c s lewis the lion the witch and the wardrobe this is the treat that seduces edmund into selling out his brother and sisters to the witch\n",
      "Summary: delight says it all\n",
      "\n",
      "\n",
      "Review: if you are looking for the secret ingredient in robitussin i believe i have found it i got this in addition to the root beer extract i ordered and made some cherry soda the flavor is very medicinal\n",
      "Summary: cough medicine\n",
      "\n",
      "\n",
      "Review: great taffy at a great price there was a wide assortment of yummy taffy delivery was very quick if your a taffy lover this is a deal\n",
      "Summary: great taffy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['Cleaned Text'][i])\n",
    "    print(\"Summary:\",data['Cleaned Summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97373\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in data['Cleaned Summary']:\n",
    "    if(len(i.split()) <= 10):\n",
    "        count = count + 1\n",
    "print(count / len(data['Cleaned Summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_summary_len = 10\n",
    "max_text_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(data['Cleaned Text'], data['Cleaned Summary'], test_size=0.2, random_state=0, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total % of rare words in vocabulary:  61.939539425502474\n",
      "Total coverage of rare words in vocabulary:  0.6908463142909037\n",
      "Size of reviews vocabulary:  18709\n"
     ]
    }
   ],
   "source": [
    "# initializing a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# any word whose word count falls below 4 would be considered a rare word\n",
    "threshold = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    total_frequency = total_frequency + value\n",
    "    if(value < threshold):\n",
    "        count = count + 1\n",
    "        frequency = frequency + value\n",
    "    \n",
    "print(\"Total % of rare words in vocabulary: \", (count / total_count) * 100)\n",
    "print(\"Total coverage of rare words in vocabulary: \", (frequency / total_frequency) * 100)\n",
    "\n",
    "# preparing a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words = total_count - count) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# converting text sequences into integer sequences\n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# padding zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen = max_text_len, padding = 'post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen = max_text_len, padding = 'post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc = x_tokenizer.num_words\n",
    "print(\"Size of reviews vocabulary: \", x_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 66.22066078636567\n",
      "Total Coverage of rare words: 3.825748591090467\n",
      "Size of summary vocabulary:  4519\n"
     ]
    }
   ],
   "source": [
    "# initialzing a tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# any word whose word count falls below 4 would be considered a rare word\n",
    "thresh = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    total_count = total_count + 1\n",
    "    total_frequency = total_frequency + value\n",
    "    if(value < thresh):\n",
    "        count = count + 1\n",
    "        frequency = frequency + value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\", (count / total_count) * 100)\n",
    "print(\"Total Coverage of rare words:\", (frequency / total_frequency) * 100)\n",
    "\n",
    "# preparing a tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer(num_words = total_count-count) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# converting text sequences into integer sequences\n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# padding zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen = max_summary_len, padding = 'post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen = max_summary_len, padding = 'post')\n",
    "\n",
    "# size of vocabulary\n",
    "y_voc = y_tokenizer.num_words\n",
    "print(\"Size of summary vocabulary: \", y_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned Summary'] = data['Cleaned Summary'].apply(lambda x : '_START_ '+ x + ' _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meat and it smells better my labrador is finicky and she appreciates this product better than most\n",
      "\n",
      "Summary: _START_ good quality dog food _END_\n"
     ]
    }
   ],
   "source": [
    "print(\"Review:\",data['Cleaned Text'][0])\n",
    "print(\"\\nSummary:\",data['Cleaned Summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06cb67651ce969e244f20f184e33f4ddb1106793adfe07cf7ddc131c95c4012f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
